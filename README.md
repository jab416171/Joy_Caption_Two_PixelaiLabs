# Joy Caption Two - PixelaiLabs Edition

<div align="center">

**Automated Image Captioning for ComfyUI with Joy Caption Alpha Two**

[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)
[![ComfyUI](https://img.shields.io/badge/ComfyUI-Custom%20Node-orange)](https://github.com/comfyanonymous/ComfyUI)

**Created by [PixelaiLabs.com](https://pixelailabs.com) | [@aiconomist](https://youtube.com/@aiconomist)**

[Installation](#installation) ‚Ä¢ [Features](#features) ‚Ä¢ [Usage](#usage) ‚Ä¢ [License](#license)

</div>


## üîî Latest Updates

### February 2026 - GGUF Support for Low VRAM
- ‚úÖ **GGUF Model Support** - Added support for quantized GGUF models using llama-cpp-python
- ‚úÖ **Q8_0 Quantization** - Use Llama-3.1-8B-Lexi-Uncensored-V2-Q8_0 GGUF model (8GB download)
- ‚úÖ **Reduced VRAM Usage** - GGUF models use significantly less VRAM than unquantized models
- ‚úÖ **Automatic Download** - GGUF models download automatically when selected
- ‚ö†Ô∏è **Note**: GGUF models use text-based prompting (no embedding injection like transformers)

### November 6, 2025 - VRAM Management Update
- ‚úÖ **Automatic VRAM Management** - Models now automatically unload from VRAM after each caption generation
- ‚úÖ **Smart Reloading** - Models reload to GPU only when needed for the next caption
- ‚úÖ **Frees ~10GB VRAM** - VRAM is released between caption generations, preventing memory buildup
- ‚úÖ **No Manual Intervention** - Everything happens automatically in the background
- üéØ **Perfect for batch processing** - Process large datasets without running out of memory

---

## üåü What is This?

A **fully automated** ComfyUI custom node that uses **Joy Caption Alpha Two** to generate high-quality image captions. Perfect for:
- LoRA/model training dataset preparation
- Image tagging and organization
- NSFW content captioning
- Batch processing large image collections

### Why This Version?

- ‚úÖ **100% Automated** - All models download automatically on first use
- ‚úÖ **Joy Caption Quality** - Uses the exact same LoRA adapter and process as the original
- ‚úÖ **Clean Output** - No "Here is a caption:" prefixes or numbered lists
- ‚úÖ **Training Ready** - Batch node outputs chronologically numbered files (1.png, 1.txt, 2.png, 2.txt, ...)
- ‚úÖ **Easy Setup** - Just install requirements and run!

---

## üì¶ Installation

### Step 1: Clone the Repository

```bash
cd ComfyUI/custom_nodes
git clone https://github.com/Pixelailabs/Joy_Caption_Two_PixelaiLabs.git
```

### Step 2: Install Requirements

```bash
cd Joy_Caption_Two_PixelaiLabs
pip install -r requirements.txt
```

### Step 3: Restart ComfyUI

That's it! On first use, the node will automatically download:
- **SigLIP vision model** (~1.5GB)
- **Joy Caption adapters** (~2.5GB total):
  - Image adapter (86 MB)
  - Custom CLIP weights (1.71 GB)
  - LoRA adapter (671 MB) - *This ensures clean caption output!*
- **Your chosen LLM** (~4-5GB when you select it)

**Total first-time download: ~6-8GB** (one-time only!)

---

## ‚ú® Features

| Feature | Description |
|---------|-------------|
| **Joy Caption LoRA** | Uses the official 671MB LoRA adapter for professional captions |
| **NSFW Capable** | Handles adult content with image embeddings (LLM truly "sees" the image) |
| **Automated Setup** | All models download automatically - no manual file placement |
| **Multiple Caption Styles** | Descriptive, Training Prompt, Booru Tags, Art Critic, etc. |
| **Adjustable Length** | From very short to very long captions |
| **Text Processing** | Replace gender/age, hair, body type, remove tattoos/jewelry |
| **Batch Processing** | Process entire folders with chronological naming for training |
| **Dual Outputs** | Advanced node outputs positive + negative prompts |

---

## üöÄ Usage

### Basic Workflow

```
[Load Image] ‚Üí [Simple LLM Caption Loader] ‚Üí [Simple LLM Caption] ‚Üí [Save Text]
                                                      ‚Üì
                                                  Caption Text
```

1. **Add "Simple LLM Caption Loader" node**
   - Select LLM: **"AUTO-DOWNLOAD: Llama-3.1-8B-Lexi-Uncensored-V2-Q8_0-GGUF"** (recommended for low VRAM)
   - Alternative: "AUTO-DOWNLOAD: Llama-3.1-8B-Lexi-Uncensored-V2-nf4" (full transformers model)
   - Models download automatically on first run

2. **Add "Simple LLM Caption" node**
   - Connect pipeline from loader
   - Connect your image
   - Choose caption type and length
   - Optional: Add LoRA trigger, text replacements

3. **Output:** Clean caption text ready to use!

### Batch Processing for Training

Perfect for preparing LoRA/model training datasets:

```
[Simple LLM Caption Loader] ‚Üí [Simple LLM Caption Batch]
                                       ‚Üì
                              Chronologically numbered files
```

**Input folder:**
```
my_photos/
  ‚îú‚îÄ‚îÄ photo1.jpg
  ‚îú‚îÄ‚îÄ image_abc.png
  ‚îî‚îÄ‚îÄ IMG_5678.jpg
```

**Output folder:**
```
training_data/
  ‚îú‚îÄ‚îÄ 1.png  ‚Üê photo1.jpg converted
  ‚îú‚îÄ‚îÄ 1.txt  ‚Üê caption for 1.png
  ‚îú‚îÄ‚îÄ 2.png  ‚Üê image_abc.png converted
  ‚îú‚îÄ‚îÄ 2.txt  ‚Üê caption for 2.png
  ‚îú‚îÄ‚îÄ 3.png  ‚Üê IMG_5678.jpg converted
  ‚îî‚îÄ‚îÄ 3.txt  ‚Üê caption for 3.png
```

Ready to use with Kohya, aitoolkit, or any training tool!

---

## üé® Caption Types

| Type | Description |
|------|-------------|
| **Descriptive** | Formal, detailed description |
| **Descriptive (Informal)** | Casual, conversational description |
| **Training Prompt** | Stable Diffusion style prompt |
| **MidJourney** | MidJourney style prompt |
| **Booru Tags** | Tag list (Danbooru/Gelbooru format) |
| **Art Critic** | Detailed analysis of composition and style |
| **Social Media** | Engaging caption for social posts |

---

## ‚öôÔ∏è Nodes

### 1. Simple LLM Caption Loader
Loads the LLM and Joy Caption adapter models.

**Parameters:**
- `llm_model` - Choose from dropdown (AUTO-DOWNLOAD options shown first)
  - **Recommended for low VRAM**: "AUTO-DOWNLOAD: Llama-3.1-8B-Lexi-Uncensored-V2-Q8_0-GGUF" (GGUF format, ~8GB download)
  - Alternative: "AUTO-DOWNLOAD: Llama-3.1-8B-Lexi-Uncensored-V2-nf4" (transformers, ~4-5GB)
  - Only shows Llama-based models (Joy Caption compatible)
  - Automatically filters out incompatible models (Florence, CLIP, etc.)
- `use_4bit` - Enable 4-bit quantization (only for non-GGUF transformers models)

**Note:** 
- Vision model (SigLIP) downloads automatically - no selection needed!
- GGUF models use llama-cpp-python and are optimized for lower VRAM usage

### 2. Simple LLM Caption
Generate captions for single images.

**Parameters:**
- `caption_type` - Style of caption
- `caption_length` - Target length (any, short, medium, long, very long)
- `lora_trigger` - Text to prepend to caption
- Text replacements (gender/age, hair, body size)
- Filters (remove tattoos, remove jewelry)

**Output:** `STRING` (caption text)

### 3. Simple LLM Caption Advanced
Advanced options with positive/negative prompt outputs.

**Additional Parameters:**
- `temperature` / `top_p` - Control randomness
- `max_new_tokens` - Maximum caption length
- `append_to_caption` - Text to add at END of caption
- `negative_prompt` - Manual negative prompt input

**Outputs:** 
- `positive_prompt` (STRING) - Final caption
- `negative_prompt` (STRING) - Your negative prompt

### 4. Simple LLM Caption Batch
Batch process folders with chronological naming for training.

**Parameters:**
- `input_directory` - Folder with images
- `output_directory` - Where to save (REQUIRED)
- All caption options from basic node

**Output:** Status message

---

## üîß System Requirements

| Requirement | Minimum | Recommended |
|-------------|---------|-------------|
| **VRAM** | 6GB (with Q8_0 GGUF) | 12GB+ |
| **RAM** | 16GB | 32GB+ |
| **Storage** | 10GB free | 20GB+ free |
| **Python** | 3.10+ | 3.11 |

**Model Options:**
- **Q8_0 GGUF**: Best for low VRAM (6-8GB), quantized model
- **4-bit Transformers**: Good for medium VRAM (8-10GB)
- **Full Transformers**: Requires high VRAM (12GB+)

---

## üìñ Credits & Attribution

This custom node is built using:

### Core Technology
- **[Joy Caption](https://github.com/fpgaminer/joycaption)** by fpgaminer - Original Joy Caption implementation
- **[Joy Caption Alpha Two](https://huggingface.co/spaces/fancyfeast/joy-caption-alpha-two)** by fancyfeast - LoRA adapter and training

### Inspiration
- **[ComfyUI_SLK_joy_caption_two](https://github.com/EvilBT/ComfyUI_SLK_joy_caption_two)** by EvilBT - ComfyUI implementation reference

### Models Used
- **SigLIP** by Google - Vision backbone
- **Llama 3.1** by Meta - Language model

---

## üìÑ License

This project is licensed under the **GNU General Public License v3.0** - see the [LICENSE](LICENSE) file for details.

**Important Notes:**
- This is a derivative work based on Joy Caption Alpha Two (GPL-3.0)
- Commercial use is allowed under GPL-3.0 terms
- Any modifications must also be released under GPL-3.0
- Please provide attribution when using or modifying this code

---

## ü§ù Support

- **Issues:** [GitHub Issues](https://github.com/YOUR_USERNAME/Joy_Caption_Two_PixelaiLabs/issues)
- **YouTube:** [@aiconomist](https://youtube.com/@aiconomist)
- **Website:** [PixelaiLabs.com](https://pixelailabs.com)

---

## üéØ Roadmap

- [ ] Support for more LLM models
- [ ] Custom LoRA adapter training
- [ ] Multi-language captions
- [ ] Video frame captioning

---

<div align="center">

**Made with ‚ù§Ô∏è by [PixelaiLabs.com](https://pixelailabs.com)**

If you find this useful, please ‚≠ê star the repository!

</div>
